services:
  agentzero:
    build:
      context: ./agent-zero
      dockerfile: Dockerfile
    container_name: agentzero_${INSTANCE_ID}
    privileged: true
    environment:
      - API_KEY_OPENAI=${API_KEY_OPENAI}
      - INSTANCE_ID=${INSTANCE_ID}
      - WEB_UI_PORT=${WEB_UI_PORT}
    volumes:
      - ${LOCAL_AGENT_DIR}:/app
    ports:
      - "80:${WEB_UI_PORT}"
      - "22:22"
    networks:
      - agent_network
    env_file:
      - ./agent-zero/.env
    depends_on:
      - ollama  # Ensure ollama starts first

  ollama:
    build:
      context: ./ollama # Path to your local Ollama Dockerfile directory
      dockerfile: Dockerfile  # Name of the Dockerfile (defaults to "Dockerfile" if not specified)
    environment:
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL}
      - OLLAMA_CHAT_MODEL=${OLLAMA_CHAT_MODEL}
      - OLLAMA_PORT=${OLLAMA_PORT:-11434}  # Default port if not set
    networks:
      - agent_network
    ports:
      - "${OLLAMA_PORT}:${OLLAMA_PORT}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: ["gpu"]


networks:
  agent_network:
    driver: bridge

volumes:
  ollama_data:
    driver: local